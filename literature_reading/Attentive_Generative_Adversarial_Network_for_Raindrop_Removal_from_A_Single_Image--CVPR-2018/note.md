# Attentive Generative Adversarial Network for Raindrop Removal from A Single Image

> - Rui Qian1, Robby T. Tan2;3∗ , Wenhan Yang1, Jiajun Su1, and Jiaying Liu
> - [**CVPR 2018**]、[[pdf](https://arxiv.org/pdf/1711.10098.pdf)]、[[code](https://github.com/rui1996/DeRaindrop)]

## 1. Introduction

**Challenge**: The raindrop-removal problem is intractable.

1. The regions which are occluded by raindrops are not given.
2. The information about the background scene of the occluded regions is completely lost for most part.
3. The problem gets worse when the raindrops are relatively large and distributed densely across the input image. 

Generative network first attempts to produce an attention map by ResNets & convolutional LSTM.

The second part of our generative network is an autoencoder.

**Contribution**: 

1. Introduce a novel method of raindrop removal.
2. Inject the attention map into both generative and discriminative networks, which is novel and works effectively in removing raindrops.

## 2. Related Work

- learns the shape of raindrops using PCA, and attempts to match a region in the test image,
  with those of the learned raindrops. 
- proposes a method that compares a synthetically generated raindrop with a patch that potentially has a raindrop. 
- uses a stereo system to detect and remove raindrops 
- Pix2Pix 

## 3. Raindrop Image Formation

**raindrop degraded image model**
$$
I = (1-M) \odot B + R
$$
> $I$: colored input image;
> $M$: the binary mask, In the mask, $M(x) = 1$ means the pixel $x$ is part of a raindrop region, and otherwise means it is part of background regions. 
> $B$: background image; 
> $R$: the effect brought by the raindrops;
> $\odot$: element-wise multiplication, 数组元素依次相乘

**Goal**: to obtain the background image $B​$ from a given input $I​$. 

## 4. Raindrop Removal using Attentive GAN

**architecture of proposed network**

![fig2](fig2.png)

**generative adversarial loss**
$$
\min_G\max_D \mathbb{E}_{R\sim p_{clean}}[\log(D(R))] + \mathbb{E}_{I\sim p_{raindrop}}[\log(1-D(G(I)))]
$$
> $G$: generative network;
> $D$: discriminative network;
> $I$: a sample drawn from our pool of images degraded by raindrops, which is the input of our generative network.
> $R$: a sample from a pool of clean natural images.

### 4.1. Generative Network

Generative network consists of two sub-networks: an **attentive-recurrent network** and a **contextual autoencoder**. 

#### Attentive-Recurrent Network

The purpose of the attentiverecurrent network is to find regions in the input image that
need to get attention. These regions are mainly the raindrop regions.

Our convolution LSTM unit consists of an **input gate** $i_t$, a **forget gate** $f_t$, an **output gate** $o_t$ as well as a **cell state** $C_t​$. 
$$
\begin{split}
&i_t = \sigma(W_{xi}*X_t+W_{hi}*H_{t-1}+W_{ci}\odot C_{t-1}+b_i) \\
&f_t = \sigma(W_{xf}*X_t+W_{hf}*H_{t-1}+W_{cf}\odot C_{t-1}+b_f) \\
&C_t = f_t \odot C_{t-1} + i_t \odot \tanh(W_{xc}*X_t + W_{hc}*H_{t-1}+b_c) \\
&o_t = \sigma(W_{xo}*X_t+W_{ho}*H_{t-1}+W_{co}\odot C_t+b_o) \\
&H_t = o_t \odot \tanh(C_t)
\end{split}
$$

> $X_t$: the features generated by ResNet
> $C_t$: encodes the cell state that will be fed to the next LSTM
> $H_t$: the output features of the LSTM unit
> $*$: the convolution operation



#### Contextual Autoencoder

### 4.2. Discriminative Network

## 5. Raindrop Dataset

## 6. Experimental Results

## 7. Conclusion



